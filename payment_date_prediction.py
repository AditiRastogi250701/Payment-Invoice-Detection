# -*- coding: utf-8 -*-
"""Payment date prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQPc3QfwNKLwqyreuJGX9qycvErH8OX1

# Payment Date Prediction

### Importing related Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

"""### Store the dataset into the Dataframe

"""

data=pd.read_csv(r'/content/dataset.csv')

"""### Check the shape of the dataframe

"""

data.shape

"""### Check the Detail information of the dataframe"""

data.info()

data.describe()

"""### Display All the column names"""

data.columns.tolist()

"""### Describe the entire dataset"""

data.describe()

"""# Data Cleaning

- Show top 5 records from the dataset
"""

data.head()

"""### Display the Null values percentage against every columns (compare to the total number of records)

- Output expected : area_business - 100% null, clear_data = 20% null, invoice_id = 0.12% null
"""

(data.isnull().sum()*100/len(data))

"""### Display Invoice_id and Doc_Id

- Note - Many of the would have same invoice_id and doc_id

"""

data['invoice_id']

data['doc_id']

"""#### Write a code to check - 'baseline_create_date',"document_create_date",'document_create_date.1' - these columns are almost same.

- Please note, if they are same, we need to drop them later


"""

(data['baseline_create_date']==data['document_create_date']).sum()

(data['baseline_create_date']==data['document_create_date.1']).sum()

(data['document_create_date']==data['document_create_date.1']).sum()

"""#### Please check, Column 'posting_id' is constant columns or not

"""

data['posting_id'].nunique(dropna=True)          #constant column

"""#### Please check 'isOpen' is a constant column and relevant column for this project or not"""

data['isOpen'].nunique(dropna=True)        #not a constant column

data['isOpen'].value_counts()

"""### Write the code to drop all the following columns from the dataframe

- 'area_business'
- "posting_id"
- "invoice_id"
- "document_create_date"
- "isOpen"
- 'document type' 
- 'document_create_date.1
"""

data.drop(['area_business','posting_id','invoice_id','document_create_date','isOpen','document type','document_create_date.1'],axis=1,inplace=True)

"""### Please check from the dataframe whether all the columns are removed or not """

data.info()

data.shape

"""### Show all the Duplicate rows from the dataframe"""

data[data.duplicated()]

"""### Display the Number of Duplicate Rows"""

len(data[data.duplicated()])

"""### Drop all the Duplicate Rows"""

data.drop_duplicates(inplace=True)

"""#### Now check for all duplicate rows now

- Note - It must be 0 by now
"""

len(data[data.duplicated()])

"""### Check for the number of Rows and Columns in your dataset"""

data.shape

"""### Find out the total count of null values in each columns"""

data.isnull().sum()

"""#Data type Conversion

### Please check the data type of each column of the dataframe
"""

data.dtypes

"""### Check the datatype format of below columns

- clear_date  
- posting_date
- due_in_date 
- baseline_create_date
"""

#already checked above

"""### converting date columns into date time formats

- clear_date  
- posting_date
- due_in_date 
- baseline_create_date


- **Note - You have to convert all these above columns into "%Y%m%d" format**
"""

data['clear_date']=pd.to_datetime(data['clear_date'])
data['posting_date']=pd.to_datetime(data['posting_date'])


data['due_in_date'] = pd.to_datetime(data['due_in_date'].astype('str'), format='%Y%m%d')
data['baseline_create_date'] = pd.to_datetime(data['baseline_create_date'].astype('str'), format='%Y%m%d')

"""### Please check the datatype of all the columns after conversion of the above 4 columns"""

data.dtypes

data.head()

"""#### the invoice_currency column contains two different categories, USD and CAD

- Please do a count of each currency 
"""

data['invoice_currency'].value_counts()

"""#### display the "total_open_amount" column value"""

data['total_open_amount']

data['total_open_amount'].value_counts()

"""### Convert all CAD into USD currency of "total_open_amount" column

- 1 CAD = 0.7 USD
- Create a new column i.e "converted_usd" and store USD and convered CAD to USD
"""

data['converted_usd']=data['total_open_amount']
data.loc[data['invoice_currency'] == 'CAD','converted_usd']=0.7*data['total_open_amount']

"""### Display the new "converted_usd" column values"""

data['converted_usd']

"""### Display year wise total number of record 

- Note -  use "business_year" column for this 
"""

data['buisness_year'].value_counts()

"""### Write the code to delete the following columns 

- 'invoice_currency'
- 'total_open_amount', 
"""

data.drop(['invoice_currency','total_open_amount'],axis=1,inplace=True)

"""### Write a code to check the number of columns in dataframe"""

data.shape

data.columns.tolist()

"""# Splitting the Dataset

### Look for all columns containing null value

- Note - Output expected is only one column
"""

data.columns[data.isnull().any()]

"""#### Find out the number of null values from the column that you got from the above code"""

data['clear_date'].isnull().sum()

"""### On basis of the above column we are spliting data into dataset

- First dataframe (refer that as maindata) only containing the rows, that have NO NULL data in that column ( This is going to be our train dataset ) 
- Second dataframe (refer that as nulldata) that contains the columns, that have Null data in that column ( This is going to be our test dataset ) 
"""

maindata=data[data['clear_date'].notnull()]
nulldata=data[data['clear_date'].isnull()]

"""### Check the number of Rows and Columns for both the dataframes """

maindata.shape

nulldata.shape

"""### Display the 5 records from maindata and nulldata dataframes"""

maindata.head()

nulldata.head()

"""## Considering the **maindata**

#### Generate a new column "Delay" from the existing columns

- Note - You are expected to create a new column 'Delay' from two existing columns, "clear_date" and "due_in_date" 
- Formula - Delay = clear_date - due_in_date
"""

maindata['Delay']=maindata['clear_date']-maindata['due_in_date']

maindata

"""### Generate a new column "avgdelay" from the existing columns

- Note - You are expected to make a new column "avgdelay" by grouping "name_customer" column with reapect to mean of the "Delay" column.
- This new column "avg_delay" is meant to store "customer_name" wise delay
- groupby('name_customer')['Delay'].mean(numeric_only=False)
- Display the new "avg_delay" column
"""

avgdelay=maindata.groupby('name_customer')['Delay'].mean(numeric_only=False)

"""You need to add the "avg_delay" column with the maindata, mapped with "name_customer" column

 - Note - You need to use map function to map the avgdelay with respect to "name_customer" column
"""

maindata['avg_delay']=maindata['name_customer'].map(avgdelay)

maindata.head()

"""### Observe that the "avg_delay" column is in days format. You need to change the format into seconds

- Days_format :  17 days 00:00:00
- Format in seconds : 1641600.0
"""

maindata['avg_delay']=round(pd.to_timedelta(maindata['avg_delay']).dt.total_seconds())

"""### Display the maindata dataframe """

maindata

"""### Since you have created the "avg_delay" column from "Delay" and "clear_date" column, there is no need of these two columns anymore 

- You are expected to drop "Delay" and "clear_date" columns from maindata dataframe 
"""

maindata.drop(['Delay','clear_date'],axis=1,inplace=True)

"""# Splitting of Train and the Test Data

### You need to split the "maindata" columns into X and y dataframe

- Note - y should have the target column i.e. "avg_delay" and the other column should be in X

- X is going to hold the source fields and y will be going to hold the target fields
"""

x=maindata.drop(['avg_delay'],axis=1)
y=maindata['avg_delay']

x

"""#### You are expected to split both the dataframes into train and test format in 60:40 ratio 

- Note - The expected output should be in "X_train", "X_loc_test", "y_train", "y_loc_test" format 
"""

from sklearn.model_selection import train_test_split

x_train,x_loc_test,y_train,y_loc_test=train_test_split(x, y, test_size=0.4, train_size=0.6, random_state=42, shuffle=True)

"""### Please check for the number of rows and columns of all the new dataframes (all 4)"""

x_train.shape

x_loc_test.shape

y_train.shape

y_loc_test.shape

"""### Now you are expected to split the "X_loc_test" and "y_loc_test" dataset into "Test" and "Validation" (as the names given below) dataframe with 50:50 format 

- Note - The expected output should be in "X_val", "X_test", "y_val", "y_test" format
"""

x_test,x_val,y_test,y_val = train_test_split(x_loc_test, y_loc_test, train_size=0.5, test_size=0.5, random_state=42, shuffle=True)

"""### Please check for the number of rows and columns of all the 4 dataframes """

x_test.shape

y_test.shape

x_val.shape

y_val.shape

"""# Exploratory Data Analysis (EDA)

### Distribution Plot of the target variable (use the dataframe which contains the target field)

- Note - You are expected to make a distribution plot for the target variable
"""

fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(y)

sns.distplot(y)

"""### You are expected to group the X_train dataset on 'name_customer' column with 'doc_id' in the x_train set

### Need to store the outcome into a new dataframe 

- Note code given for groupby statement- X_train.groupby(by=['name_customer'], as_index=False)['doc_id'].count()
"""

data1=x_train.groupby(['name_customer'], as_index=False)['doc_id'].count()

data1

"""### You can make another distribution plot of the "doc_id" column from x_train"""

sns.distplot(x_train['doc_id'])

"""#### Create a Distribution plot only for business_year and a seperate distribution plot of "business_year" column along with the doc_id" column

"""

sns.distplot(x_train['buisness_year'])

sns.distplot(x_train['buisness_year'])
sns.distplot(x_train['doc_id'])

"""# Feature Engineering

### Display and describe the X_train dataframe
"""

x_train

x_train.describe()

"""#### The "business_code" column inside X_train, is a categorical column, so you need to perform Labelencoder on that particular column

- Note - call the Label Encoder from sklearn library and use the fit() function on "business_code" column
- Note - Please fill in the blanks (two) to complete this code
"""

from sklearn.preprocessing import LabelEncoder
business_coder = LabelEncoder()
business_coder.fit(x_train['business_code'])

"""#### You are expected to store the value into a new column i.e. "business_code_enc"

- Note - For Training set you are expected to use fit_trainsform()
- Note - For Test set you are expected to use the trainsform()
- Note - For Validation set you are expected to use the trainsform()


- Partial code is provided, please fill in the blanks 
"""

x_train['business_code_enc'] = business_coder.fit_transform(x_train['business_code'])

x_val['business_code_enc'] = business_coder.transform(x_val['business_code'])
x_test['business_code_enc'] = business_coder.transform(x_test['business_code'])

"""### Display "business_code" and "business_code_enc" together from X_train dataframe """

x_train[['business_code','business_code_enc']]

"""#### Create a function called "custom" for dropping the columns 'business_code' from train, test and validation dataframe

- Note - Fill in the blank to complete the code
"""

def custom (col ,traindf = x_train,valdf = x_val,testdf = x_test):
    traindf.drop(col, axis =1,inplace=True)
    valdf.drop(col,axis=1 , inplace=True)
    testdf.drop(col,axis=1 , inplace=True)

    return traindf,valdf ,testdf

"""### Call the function by passing the column name which needed to be dropped from train, test and validation dataframes. Return updated dataframes to be stored in X_train ,X_val, X_test  

- Note = Fill in the blank to complete the code 
"""

x_train,x_val,x_test = custom(['business_code'])

"""### Manually replacing str values with numbers, Here we are trying manually replace the customer numbers with some specific values like, 'CCCA' as 1, 'CCU' as 2 and so on. Also we are converting the datatype "cust_number" field to int type.

- We are doing it for all the three dataframes as shown below. This is fully completed code. No need to modify anything here 


"""

x_train['cust_number'] = x_train['cust_number'].str.replace('CCCA',"1").str.replace('CCU',"2").str.replace('CC',"3").astype(int)
x_test['cust_number'] = x_test['cust_number'].str.replace('CCCA',"1").str.replace('CCU',"2").str.replace('CC',"3").astype(int)
x_val['cust_number'] = x_val['cust_number'].str.replace('CCCA',"1").str.replace('CCU',"2").str.replace('CC',"3").astype(int)

"""#### It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]. Unknown will be added in fit and transform will take care of new item. It gives unknown class id.

#### This will fit the encoder for all the unique values and introduce unknown value

- Note - Keep this code as it is, we will be using this later on.  
"""

#For encoding unseen labels
class EncoderExt(object):
    def __init__(self):
        self.label_encoder = LabelEncoder()
    def fit(self, data_list):
        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])
        self.classes_ = self.label_encoder.classes_
        return self
    def transform(self, data_list):
        new_data_list = list(data_list)
        for unique_item in np.unique(data_list):
            if unique_item not in self.label_encoder.classes_:
                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]
        return self.label_encoder.transform(new_data_list)

"""### Use the user define Label Encoder function called "EncoderExt" for the "name_customer" column

- Note - Keep the code as it is, no need to change
"""

label_encoder = EncoderExt()
label_encoder.fit(x_train['name_customer'])
x_train['name_customer_enc']=label_encoder.transform(x_train['name_customer'])
x_val['name_customer_enc']=label_encoder.transform(x_val['name_customer'])
x_test['name_customer_enc']=label_encoder.transform(x_test['name_customer'])

"""### As we have created the a new column "name_customer_enc", so now drop "name_customer" column from all three dataframes

- Note - Keep the code as it is, no need to change
"""

x_train ,x_val, x_test = custom(['name_customer'])

"""### Using Label Encoder for the "cust_payment_terms" column

- Note - Keep the code as it is, no need to change
"""

label_encoder1 = EncoderExt()
label_encoder1.fit(x_train['cust_payment_terms'])
x_train['cust_payment_terms_enc']=label_encoder1.transform(x_train['cust_payment_terms'])
x_val['cust_payment_terms_enc']=label_encoder1.transform(x_val['cust_payment_terms'])
x_test['cust_payment_terms_enc']=label_encoder1.transform(x_test['cust_payment_terms'])

x_train ,x_val, x_test = custom(['cust_payment_terms'])

"""## Check the datatype of all the columns of Train, Test and Validation dataframes realted to X

- Note - You are expected yo use dtype
"""

x_train.dtypes

x_val.dtypes

x_test.dtypes

"""### From the above output you can notice their are multiple date columns with datetime format

### In order to pass it into our model, we need to convert it into float format

### You need to extract day, month and year from the "posting_date" column 

1.   Extract days from "posting_date" column and store it into a new column "day_of_postingdate" for train, test and validation dataset 
2.   Extract months from "posting_date" column and store it into a new column "month_of_postingdate" for train, test and validation dataset
3.   Extract year from "posting_date" column and store it into a new column "year_of_postingdate" for train, test and validation dataset 



- Note - You are supposed yo use 

*   dt.day
*   dt.month
*   dt.year
"""

x_train['day_of_postingdate'] = x_train['posting_date'].dt.day
x_train['month_of_postingdate'] = x_train['posting_date'].dt.month
x_train['year_of_postingdate'] = x_train['posting_date'].dt.year

x_val['day_of_postingdate'] = x_val['posting_date'].dt.day
x_val['month_of_postingdate'] = x_val['posting_date'].dt.month
x_val['year_of_postingdate'] = x_val['posting_date'].dt.year


x_test['day_of_postingdate'] = x_test['posting_date'].dt.day
x_test['month_of_postingdate'] = x_test['posting_date'].dt.month
x_test['year_of_postingdate'] = x_test['posting_date'].dt.year

"""### pass the "posting_date" column into the Custom function for train, test and validation dataset"""

x_train ,x_val, x_test = custom(['posting_date'])

"""### You need to extract day, month and year from the "baseline_create_date" column 

1.   Extract days from "baseline_create_date" column and store it into a new column "day_of_createdate" for train, test and validation dataset 
2.   Extract months from "baseline_create_date" column and store it into a new column "month_of_createdate" for train, test and validation dataset
3.   Extract year from "baseline_create_date" column and store it into a new column "year_of_createdate" for train, test and validation dataset 



- Note - You are supposed yo use 

*   dt.day
*   dt.month
*   dt.year


- Note - Do as it is been shown in the previous two code boxes

### Extracting Day, Month, Year for 'baseline_create_date' column
"""

x_train['day_of_Baseline'] = x_train['baseline_create_date'].dt.day
x_train['month_of_Baseline'] = x_train['baseline_create_date'].dt.month
x_train['year_of_Baseline'] = x_train['baseline_create_date'].dt.year

x_val['day_of_Baseline'] = x_val['baseline_create_date'].dt.day
x_val['month_of_Baseline'] = x_val['baseline_create_date'].dt.month
x_val['year_of_Baseline'] = x_val['baseline_create_date'].dt.year


x_test['day_of_Baseline'] = x_test['baseline_create_date'].dt.day
x_test['month_of_Baseline'] = x_test['baseline_create_date'].dt.month
x_test['year_of_Baseline'] = x_test['baseline_create_date'].dt.year

"""### pass the "baseline_create_date" column into the Custom function for train, test and validation dataset"""

x_train,  x_val, x_test=custom(['baseline_create_date'])

"""### You need to extract day, month and year from the "due_in_date" column 

1.   Extract days from "due_in_date" column and store it into a new column "day_of_due" for train, test and validation dataset 
2.   Extract months from "due_in_date" column and store it into a new column "month_of_due" for train, test and validation dataset
3.   Extract year from "due_in_date" column and store it into a new column "year_of_due" for train, test and validation dataset 



- Note - You are supposed yo use 

*   dt.day
*   dt.month
*   dt.year

- Note - Do as it is been shown in the previous code
"""

x_train['day_of_due'] = x_train['due_in_date'].dt.day
x_train['month_of_due'] = x_train['due_in_date'].dt.month
x_train['year_of_due'] = x_train['due_in_date'].dt.year

x_val['day_of_due'] = x_val['due_in_date'].dt.day
x_val['month_of_due'] = x_val['due_in_date'].dt.month
x_val['year_of_due'] = x_val['due_in_date'].dt.year


x_test['day_of_due'] = x_test['due_in_date'].dt.day
x_test['month_of_due'] = x_test['due_in_date'].dt.month
x_test['year_of_due'] = x_test['due_in_date'].dt.year

"""pass the "due_in_date" column into the Custom function for train, test and validation dataset"""

x_train,  x_val, x_test=custom(['due_in_date'])

"""### Check for the datatypes for train, test and validation set again

- Note - all the data type should be in either int64 or float64 format 

"""

x_train.dtypes

x_test.dtypes

x_val.dtypes

"""# Feature Selection

### Filter Method

- Calling the VarianceThreshold Function 
- Note - Keep the code as it is, no need to change
"""

from sklearn.feature_selection import VarianceThreshold
constant_filter = VarianceThreshold(threshold=0)
constant_filter.fit(x_train)
len(x_train.columns[constant_filter.get_support()])

"""- Note - Keep the code as it is, no need to change 

"""

constant_columns = [column for column in x_train.columns
                    if column not in x_train.columns[constant_filter.get_support()]]
print(len(constant_columns))

"""- transpose the feature matrice
- print the number of duplicated features
- select the duplicated features columns names

- Note - Keep the code as it is, no need to change 

"""

x_train_T = x_train.T
print(x_train_T.duplicated().sum())
duplicated_columns = x_train_T[x_train_T.duplicated()].index.values

"""### Filtering depending upon correlation matrix value
- We have created a function called handling correlation which is going to return fields based on the correlation matrix value with a threshold of 0.8

- Note - Keep the code as it is, no need to change 
"""

def handling_correlation(X_train,threshold=0.8):
    corr_features = set()
    corr_matrix = X_train.corr()
    for i in range(len(corr_matrix .columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) >threshold:
                colname = corr_matrix.columns[i]
                corr_features.add(colname)
    return list(corr_features)

"""- Note : Here we are trying to find out the relevant fields, from X_train
- Please fill in the blanks to call handling_correlation() function with a threshold value of 0.85
"""

train=x_train.copy()
handling_correlation(train.copy(),0.85)

"""### Heatmap for X_train

- Note - Keep the code as it is, no need to change
"""

colormap = plt.cm.RdBu
plt.figure(figsize=(14,12))
plt.title('Pearson Correlation of Features', y=1.05, size=20)
sns.heatmap(x_train.merge(y_train , on = x_train.index ).corr(),linewidths=0.1,vmax=1.0, 
            square=True, cmap='gist_rainbow_r', linecolor='white', annot=True)

"""#### Calling variance threshold for threshold value = 0.8

- Note -  Fill in the blanks to call the appropriate method
"""

from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(0.8)
sel.fit(x_train)

sel.variances_

"""### Features columns are 
- 'year_of_createdate' 
- 'year_of_due'
- 'day_of_createdate'
- 'year_of_postingdate'
- 'month_of_due'
- 'month_of_createdate'

# Modelling 

#### Now you need to compare with different machine learning models, and needs to find out the best predicted model

- Linear Regression
- Decision Tree Regression
- Random Forest Regression
- Support Vector Regression
- Extreme Gradient Boost Regression

### You need to make different blank list for different evaluation matrix 

- MSE
- R2
- Algorithm
"""

MSE_Score = []
R2_Score = []
Algorithm = []
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

"""### You need to start with the baseline model Linear Regression

- Step 1 : Call the Linear Regression from sklearn library
- Step 2 : make an object of Linear Regression 
- Step 3 : fit the X_train and y_train dataframe into the object 
- Step 4 : Predict the output by passing the X_test Dataset into predict function




- Note - Append the Algorithm name into the algorithm list for tracking purpose
"""

from sklearn.linear_model import LinearRegression
Algorithm.append('LinearRegression')
lr = LinearRegression()
lr.fit(x_train, y_train)
predicted= lr.predict(x_test)

"""### Check for the 

- Mean Square Error
- R Square Error 

for y_test and predicted dataset and store those data inside respective list for comparison 
"""

MSE_Score.append(mean_squared_error(y_test, predicted,squared=True))
R2_Score.append(r2_score(y_test, predicted))

"""### Check the same for the Validation set also """

predict_test=lr.predict(x_val)
mean_squared_error(y_val, predict_test, squared=True)

"""### Display The Comparison Lists"""

for i in Algorithm, MSE_Score, R2_Score:
    print(i,end=',')

"""### You need to start with the baseline model Support Vector Regression

- Step 1 : Call the Support Vector Regressor from sklearn library
- Step 2 : make an object of SVR
- Step 3 : fit the X_train and y_train dataframe into the object 
- Step 4 : Predict the output by passing the X_test Dataset into predict function




- Note - Append the Algorithm name into the algorithm list for tracking purpose
"""

from sklearn.svm import SVR
Algorithm.append('SupportVectorRegression')

svr = SVR()
svr.fit(x_train, y_train)

predicted= svr.predict(x_test)

"""### Check for the 

- Mean Square Error
- R Square Error 

for "y_test" and "predicted" dataset and store those data inside respective list for comparison 
"""

MSE_Score.append(mean_squared_error(y_test, predicted,squared=True))
R2_Score.append(r2_score(y_test, predicted))

"""### Check the same for the Validation set also """

predict_test= svr.predict(x_val)
mean_squared_error(y_val, predict_test, squared=True)

"""### Display The Comparison Lists"""

for i in Algorithm, MSE_Score, R2_Score:
    print(i,end=',')

"""### Your next model would be Decision Tree Regression

- Step 1 : Call the Decision Tree Regressor from sklearn library
- Step 2 : make an object of Decision Tree
- Step 3 : fit the X_train and y_train dataframe into the object 
- Step 4 : Predict the output by passing the X_test Dataset into predict function




- Note - Append the Algorithm name into the algorithm list for tracking purpose
"""

from sklearn.tree import DecisionTreeRegressor
Algorithm.append('Decision Tree Regression')

dtr = DecisionTreeRegressor()
dtr.fit(x_train, y_train)

predicted= dtr.predict(x_test)

"""### Check for the 

- Mean Square Error
- R Square Error 

for y_test and predicted dataset and store those data inside respective list for comparison 
"""

MSE_Score.append(mean_squared_error(y_test, predicted,squared=True))
R2_Score.append(r2_score(y_test, predicted))

"""### Check the same for the Validation set also """

predict_test= dtr.predict(x_val)
mean_squared_error(y_val, predict_test, squared=True)

"""### Display The Comparison Lists"""

for i in Algorithm, MSE_Score, R2_Score:
    print(i,end=',')

"""### Your next model would be Random Forest Regression

- Step 1 : Call the Random Forest Regressor from sklearn library
- Step 2 : make an object of Random Forest
- Step 3 : fit the X_train and y_train dataframe into the object 
- Step 4 : Predict the output by passing the X_test Dataset into predict function




- Note - Append the Algorithm name into the algorithm list for tracking purpose
"""

from sklearn.ensemble import RandomForestRegressor
Algorithm.append('Random Forest Regression')

rfr = RandomForestRegressor()
rfr.fit(x_train, y_train)

predicted=rfr.predict(x_test)

"""### Check for the 

- Mean Square Error
- R Square Error 

for y_test and predicted dataset and store those data inside respective list for comparison 
"""

MSE_Score.append(mean_squared_error(y_test, predicted,squared=True))
R2_Score.append(r2_score(y_test, predicted))

"""### Check the same for the Validation set also """

predict_test= rfr.predict(x_val)
mean_squared_error(y_val, predict_test, squared=True)

"""### Display The Comparison Lists

"""

for i in Algorithm, MSE_Score, R2_Score:
    print(i,end=',')

"""### The last but not the least model would be XGBoost or Extreme Gradient Boost Regression

- Step 1 : Call the XGBoost Regressor from xgb library
- Step 2 : make an object of Xgboost
- Step 3 : fit the X_train and y_train dataframe into the object 
- Step 4 : Predict the output by passing the X_test Dataset into predict function




- Note - Append the Algorithm name into the algorithm list for tracking purpose### Extreme Gradient Boost Regression
- Note -  No need to change the code 
"""

import xgboost as xgb
Algorithm.append('XGB Regressor')

xgbr = xgb.XGBRegressor()
xgbr.fit(x_train, y_train)

predicted = xgbr.predict(x_test)

"""### Check for the 

- Mean Square Error
- R Square Error 

for y_test and predicted dataset and store those data inside respective list for comparison 
"""

MSE_Score.append(mean_squared_error(y_test, predicted,squared=True))
R2_Score.append(r2_score(y_test, predicted))

"""### Check the same for the Validation set also """

predict_test= xgbr.predict(x_val)
mean_squared_error(y_val, predict_test, squared=True)

"""### Display The Comparison Lists

"""

for i in Algorithm, MSE_Score, R2_Score:
    print(i,end=',')

"""## You need to make the comparison list into a comparison dataframe """

Scores = pd.DataFrame(list(zip(Algorithm, MSE_Score,R2_Score)), columns = ['Algorithm','MSE_Score','R2_Score'])

"""## Now from the Comparison table, you need to choose the best fit model

- Step 1 - Fit X_train and y_train inside the model 
- Step 2 - Predict the X_test dataset
- Step 3 - Predict the X_val dataset


- Note - No need to change the code
"""

np.argmin(MSE_Score)

Algorithm[3]

regressor = RandomForestRegressor()
regressor.fit(x_train, y_train)
predicted_test_final = regressor.predict(x_test)
predicted_val_final = regressor.predict(x_val)

"""### Calculate the Mean Square Error for test dataset

- Note - No need to change the code
"""

mean_squared_error(y_test,predicted_test_final,squared=True)

"""### Calculate the mean Square Error for validation dataset"""

mean_squared_error(y_val, predicted_val_final, squared=True)

"""### Calculate the R2 score for test"""

r2_score(y_test, predicted_test_final)

"""### Calculate the R2 score for Validation"""

r2_score(y_val, predicted_val_final)

"""### Calculate the Accuracy for train Dataset """

train_accuracy=round(regressor.score(x_train,y_train)*100,2)
print(train_accuracy,'%')

"""### Calculate the accuracy for validation"""

val_accuracy=round(regressor.score(x_val,y_val)*100,2)
print(val_accuracy,'%')

"""### Calculate the accuracy for test"""

test_accuracy=round(regressor.score(x_test,y_test)*100,2)
print(test_accuracy,'%')

"""## Specify the reason behind choosing your machine learning model 

- Note : Provide your answer as a text here

## Now you need to pass the Nulldata dataframe into this machine learning model

#### In order to pass this Nulldata dataframe into the ML model, we need to perform the following

- Step 1 : Label Encoding 
- Step 2 : Day, Month and Year extraction 
- Step 3 : Change all the column data type into int64 or float64
- Step 4 : Need to drop the useless columns

### Display the Nulldata
"""

nulldata

"""### Check for the number of rows and columns in the nulldata"""

nulldata.shape

"""### Check the Description and Information of the nulldata """

nulldata.describe()

"""### Storing the Nulldata into a different dataset 
# for BACKUP
"""

nulldata1=nulldata.copy()

nulldata1

"""### Call the Label Encoder for Nulldata

- Note - you are expected to fit "business_code" as it is a categorical variable
- Note - No need to change the code
"""

from sklearn.preprocessing import LabelEncoder
business_codern = LabelEncoder()
business_codern.fit(nulldata['business_code'])
nulldata['business_code_enc'] = business_codern.transform(nulldata['business_code'])

"""### Now you need to manually replacing str values with numbers
- Note - No need to change the code
"""

nulldata['cust_number'] = nulldata['cust_number'].str.replace('CCCA',"1").str.replace('CCU',"2").str.replace('CC',"3").astype(int)

"""## You need to extract day, month and year from the "clear_date", "posting_date", "due_in_date", "baseline_create_date" columns


##### 1.   Extract day from "clear_date" column and store it into 'day_of_cleardate'
##### 2.   Extract month from "clear_date" column and store it into 'month_of_cleardate'
##### 3.   Extract year from "clear_date" column and store it into 'year_of_cleardate'



##### 4.   Extract day from "posting_date" column and store it into 'day_of_postingdate'
##### 5.   Extract month from "posting_date" column and store it into 'month_of_postingdate'
##### 6.   Extract year from "posting_date" column and store it into 'year_of_postingdate'




##### 7.   Extract day from "due_in_date" column and store it into 'day_of_due'
##### 8.   Extract month from "due_in_date" column and store it into 'month_of_due'
##### 9.   Extract year from "due_in_date" column and store it into 'year_of_due'




##### 10.   Extract day from "baseline_create_date" column and store it into 'day_of_createdate'
##### 11.   Extract month from "baseline_create_date" column and store it into 'month_of_createdate'
##### 12.   Extract year from "baseline_create_date" column and store it into 'year_of_createdate'




- Note - You are supposed To use - 

*   dt.day
*   dt.month
*   dt.year
"""

nulldata['day_of_postingdate'] = nulldata['posting_date'].dt.day
nulldata['month_of_postingdate'] = nulldata['posting_date'].dt.month
nulldata['year_of_postingdate'] = nulldata['posting_date'].dt.year

nulldata['day_of_due'] = nulldata['due_in_date'].dt.day
nulldata['month_of_due'] = nulldata['due_in_date'].dt.month
nulldata['year_of_due'] = nulldata['due_in_date'].dt.year

nulldata['day_of_createdate'] =nulldata['baseline_create_date'].dt.day
nulldata['month_of_createdate'] = nulldata['baseline_create_date'].dt.month
nulldata['year_of_createdate'] = nulldata['baseline_create_date'].dt.year

nulldata['day_of_cleardate'] =nulldata['clear_date'].dt.day
nulldata['month_of_cleardate'] = nulldata['clear_date'].dt.month
nulldata['year_of_cleardate'] = nulldata['clear_date'].dt.year

"""### Use Label Encoder1 of all the following columns - 
- 'cust_payment_terms' and store into 'cust_payment_terms_enc'
- 'business_code' and store into 'business_code_enc'
- 'name_customer' and store into 'name_customer_enc'

Note - No need to change the code
"""

nulldata['cust_payment_terms_enc']=label_encoder1.transform(nulldata['cust_payment_terms'])
nulldata['business_code_enc']=label_encoder1.transform(nulldata['business_code'])
nulldata['name_customer_enc']=label_encoder.transform(nulldata['name_customer'])

"""### Check for the datatypes of all the columns of Nulldata"""

nulldata.dtypes

"""### Now you need to drop all the unnecessary columns - 

- 'business_code'
- "baseline_create_date"
- "due_in_date"
- "posting_date"
- "name_customer"
- "clear_date"
- "cust_payment_terms"
- 'day_of_cleardate'
- "month_of_cleardate"
- "year_of_cleardate"
"""

nulldata.drop(['business_code','baseline_create_date','due_in_date','posting_date','name_customer','clear_date','cust_payment_terms','day_of_cleardate','month_of_cleardate','year_of_cleardate'],axis=1,inplace=True)

"""### Check the information of the "nulldata" dataframe"""

nulldata.shape

nulldata.describe()

"""### Compare "nulldata" with the "X_test" dataframe 

- use info() method
"""

nulldata.info()

x_test.info()

"""### You must have noticed that there is a mismatch in the column sequence while compairing the dataframes

- Note - In order to fed into the machine learning model, you need to edit the sequence of "nulldata", similar to the "X_test" dataframe

- Display all the columns of the X_test dataframe 
- Display all the columns of the Nulldata dataframe 
- Store the Nulldata with new sequence into a new dataframe 


- Note - The code is given below, no need to change
"""

x_test.columns

nulldata.columns

nulldata2=nulldata[['cust_number', 'buisness_year', 'doc_id', 'converted_usd','business_code_enc', 'name_customer_enc', 'cust_payment_terms_enc','day_of_postingdate', 'month_of_postingdate', 'year_of_postingdate','day_of_createdate', 'month_of_createdate', 'year_of_createdate',
       'day_of_due', 'month_of_due', 'year_of_due']]

"""### Display the Final Dataset"""

nulldata2

"""### Now you can pass this dataset into you final model and store it into "final_result""""

final_result=regressor.predict(nulldata2)

"""### you need to make the final_result as dataframe, with a column name "avg_delay"

- Note - No need to change the code
"""

final_result = pd.DataFrame(final_result,columns=['avg_delay'])

"""### Display the "avg_delay" column"""

final_result['avg_delay']

"""### Now you need to merge this final_result dataframe with the BACKUP of "nulldata" Dataframe which we have created in earlier steps"""

nulldata1.reset_index(drop=True,inplace=True)
Final = nulldata1.merge(final_result , on = nulldata.index )

"""### Display the "Final" dataframe """

Final

"""### Check for the Number of Rows and Columns in your "Final" dataframe """

Final.shape

"""### Now, you need to do convert the below fields back into date and time format 

- Convert "due_in_date" into datetime format
- Convert "avg_delay" into datetime format
- Create a new column "clear_date" and store the sum of "due_in_date" and "avg_delay"
- display the new "clear_date" column
- Note - Code is given below, no need to change 
"""

Final['clear_date'] = pd.to_datetime(Final['due_in_date']) + pd.to_timedelta(Final['avg_delay'], unit='s')

"""### Display the "clear_date" column"""

Final['clear_date']

"""### Convert the average delay into number of days format 

- Note - Formula = avg_delay//(24 * 3600)
- Note - full code is given for this, no need to change 
"""

Final['avg_delay'] = Final.apply(lambda row: row.avg_delay//(24 * 3600), axis = 1)

"""### Display the "avg_delay" column """

Final['avg_delay']

"""### Now you need to convert average delay column into bucket

- Need to perform binning 
- create a list of bins i.e. bins= [0,15,30,45,60,100]
- create a list of labels i.e. labels = ['0-15','16-30','31-45','46-60','Greatar than 60']
- perform binning by using cut() function from "Final" dataframe


- Please fill up the first two rows of the code
"""

bins= [0,15,30,45,60,100]
labels = ['0-15','16-30','31-45','46-60','Greatar than 60']
Final['Aging Bucket'] = pd.cut(Final['avg_delay'], bins=bins, labels=labels, right=False)

"""### Now you need to drop "key_0" and "avg_delay" columns from the "Final" Dataframe"""

Final.drop(['key_0','avg_delay'],axis=1,inplace=True)

"""### Display the count of each categoty of new "Aging Bucket" column """

Final['Aging Bucket'].value_counts()

"""### Display your final dataset with aging buckets """

Final

"""### Store this dataframe into the .csv format"""

Final.to_csv('Final Dataset.csv', index=False)

"""# END OF THE PROJECT"""